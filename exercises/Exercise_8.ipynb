{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0KFufDwz_ur"
      },
      "source": [
        "Copyright (c) Technical University of Munich. All Rights Reserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd_ZqAh_0FhO"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        " <td><img src=\"https://raw.githubusercontent.com/CS4MS/CS4MS_exercise/main/images/logo_CS_MS_final.png\" height=200 style=\"float: left;\"></td>\n",
        "    <td>\n",
        "        <h1>Computer Science for Medical Students</h1>\n",
        "        <h2>Exercise 7: Vision-Languag Model for Radiology</h2>\n",
        "        <h3>Matthias Keicher &amp; Chantal Pellegrini</h3>\n",
        "        <a href=\"https://www.cs.cit.tum.de/camp/research/vision-language/\" target=\"_blank\">Our Vision-Language Research Group @ CAMP</a>\n",
        "        </br></br>\n",
        "        <a href=\"https://github.com/CS4MS/\" target=\"_blank\">CS4MS GitHub Repository</a>\n",
        "    </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALTFUW7Sp7wN"
      },
      "source": [
        "# Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poqhWSKir7aY"
      },
      "source": [
        "## Recap CLIP\n",
        "![CLIP: Contrastive Language-Image Pretraining](https://github.com/openai/CLIP/raw/main/CLIP.png)\n",
        "\n",
        "[OpenAI blog post with more details](https://openai.com/research/clip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bCW9Q3-r-es"
      },
      "source": [
        "## BioVil: Making the Most of Text Semantics to Improve Biomedical Vision–Language Processing\n",
        "\n",
        "![BioVil architecture](https://www.microsoft.com/en-us/research/uploads/prod/2022/07/BioVIL-1024x328.png)\n",
        "- Contrastive language-image pretraining on chest X-rays and corresponding radiology reports proposed by Microsoft Research [1].\n",
        "- Trained on the [MIMIC-CXR dataset](https://physionet.org/content/mimic-cxr/2.0.0/) with 377,110 radiographs and corresponding radiology reports.\n",
        "\n",
        "- [BioVil blog post with more details](https://www.microsoft.com/en-us/research/publication/making-the-most-of-text-semantics-to-improve-biomedical-vision-language-processing/)\n",
        "\n",
        "[1] Boecking, Benedikt, et al. \"Making the most of text semantics to improve biomedical vision–language processing.\" European conference on computer vision. Cham: Springer Nature Switzerland, 2022.\n",
        "\n",
        "<br >\n",
        "\n",
        "### Image Encoder\n",
        "![CNN](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
        "![ResNet50](https://upload.wikimedia.org/wikipedia/commons/9/98/ResNet50.png)\n",
        "\n",
        "- [ResNet50](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) CNN architecture\n",
        "- Pretrained on chest X-rays with contrastive pretraining ([SimCLR](http://proceedings.mlr.press/v119/chen20j.html))\n",
        "\n",
        "<br >\n",
        "\n",
        "### Text Encoder\n",
        "\n",
        "![BioVil CXR-BERT Text Encoder](https://www.microsoft.com/en-us/research/uploads/prod/2022/07/CXR-BERT.png)\n",
        "\n",
        "- Based on [BERT Transformer encoder architecture](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
        "- Pretrained on PubMed articles, [MIMIC clinical notes](https://physionet.org/content/mimiciii) and MIMIC-CXR radiology reports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ckUY4hpLvy"
      },
      "source": [
        "# Vision-Language Model\n",
        "Installation of dependencies and loading of text and image encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGJ6gRRxphzU"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU9HbaUv-X1X"
      },
      "outputs": [],
      "source": [
        "# Installation of libraries needed for transformer-based language models and timm for CNN image encoders\n",
        "%pip install transformers timm --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l60wyhVIp2Kq"
      },
      "source": [
        "## Loading Image Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "o8y3r5yNWyO6"
      },
      "outputs": [],
      "source": [
        "#@title BioVil ResNet\n",
        "#  -------------------------------------------------------------------------------------------\n",
        "#  Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n",
        "#  -------------------------------------------------------------------------------------------\n",
        "\n",
        "# Adopted from https://github.com/microsoft/hi-ml/tree/c606808b20c88d6e2cc388bd650abf34ccae17cf/hi-ml-multimodal/src/health_multimodal/image/model\n",
        "\n",
        "import math\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from contextlib import contextmanager\n",
        "from functools import partial\n",
        "from enum import Enum, unique\n",
        "from typing import Callable, Optional, Any, List, Tuple, Type, Union, Set, Generator, Sequence\n",
        "from abc import ABC, abstractmethod\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
        "from timm.models.layers import DropPath, Mlp, trunc_normal_\n",
        "\n",
        "\n",
        "def get_module_device(module: torch.nn.Module) -> torch.device:\n",
        "    \"\"\"\n",
        "    Returns the device of the module\n",
        "    \"\"\"\n",
        "    device = next(module.parameters()).device  # type: ignore\n",
        "    assert isinstance(device, torch.device)\n",
        "\n",
        "    return device\n",
        "\n",
        "@dataclass\n",
        "class ImageModelOutput:\n",
        "    img_embedding: torch.Tensor\n",
        "    patch_embeddings: torch.Tensor\n",
        "    projected_global_embedding: torch.Tensor\n",
        "    class_logits: torch.Tensor\n",
        "    projected_patch_embeddings: torch.Tensor\n",
        "\n",
        "\n",
        "@unique\n",
        "class ImageEncoderType(str, Enum):\n",
        "    RESNET18 = \"resnet18\"\n",
        "    RESNET50 = \"resnet50\"\n",
        "    RESNET18_MULTI_IMAGE = \"resnet18_multi_image\"\n",
        "    RESNET50_MULTI_IMAGE = \"resnet50_multi_image\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_members(cls, multi_image_encoders_only: bool) -> List[ImageEncoderType]:\n",
        "        if multi_image_encoders_only:\n",
        "            return [cls.RESNET18_MULTI_IMAGE, cls.RESNET50_MULTI_IMAGE]\n",
        "        else:\n",
        "            return [member for member in cls]\n",
        "\n",
        "\n",
        "@unique\n",
        "class ImageEncoderWeightTypes(str, Enum):\n",
        "    RANDOM = \"random\"\n",
        "    IMAGENET = \"imagenet\"\n",
        "    BIOVIL = \"biovil\"\n",
        "    BIOVIL_T = \"biovil_t\"\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully connected layers to map between image embeddings and projection space where pairs of images are compared.\n",
        "\n",
        "    :param input_dim: Input embedding feature size\n",
        "    :param hidden_dim: Hidden layer size in MLP\n",
        "    :param output_dim: Output projection size\n",
        "    :param use_1x1_convs: Use 1x1 conv kernels instead of 2D linear transformations for speed and memory efficiency.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, input_dim: int, output_dim: int, hidden_dim: Optional[int] = None, use_1x1_convs: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if use_1x1_convs:\n",
        "            linear_proj_1_args = {'in_channels': input_dim, 'out_channels': hidden_dim, 'kernel_size': 1, 'bias': False}\n",
        "            linear_proj_2_args = {'in_channels': hidden_dim, 'out_channels': output_dim, 'kernel_size': 1, 'bias': True}\n",
        "            normalisation_layer: Callable = nn.BatchNorm2d\n",
        "            projection_layer: Callable = nn.Conv2d\n",
        "        else:\n",
        "            linear_proj_1_args = {'in_features': input_dim, 'out_features': hidden_dim, 'bias': False}\n",
        "            linear_proj_2_args = {'in_features': hidden_dim, 'out_features': output_dim, 'bias': True}\n",
        "            normalisation_layer = nn.BatchNorm1d\n",
        "            projection_layer = nn.Linear\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.input_dim = input_dim\n",
        "        if hidden_dim is not None:\n",
        "            self.model = nn.Sequential(\n",
        "                projection_layer(**linear_proj_1_args),\n",
        "                normalisation_layer(hidden_dim),\n",
        "                nn.ReLU(inplace=True),\n",
        "                projection_layer(**linear_proj_2_args),\n",
        "            )\n",
        "        else:\n",
        "            self.model = nn.Linear(input_dim, output_dim)  # type: ignore\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"forward pass of the multi-layer perceptron\"\"\"\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    \"\"\"Torch module for multi-task classification heads. We create a separate classification head\n",
        "    for each task and perform a forward pass on each head independently in forward(). Classification\n",
        "    heads are instances of `MLP`.\n",
        "\n",
        "    :param input_dim: Number of dimensions of the input feature map.\n",
        "    :param classifier_hidden_dim: Number of dimensions of hidden features in the MLP.\n",
        "    :param num_classes: Number of output classes per task.\n",
        "    :param num_tasks: Number of classification tasks or heads required.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, classifier_hidden_dim: Optional[int], num_classes: int, num_tasks: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        for task in range(num_tasks):\n",
        "            # TODO check if softmax not needed here.\n",
        "            setattr(self, \"fc_\" + str(task), MLP(input_dim, output_dim=num_classes, hidden_dim=classifier_hidden_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Returns [batch_size, num_tasks, num_classes] tensor of logits.\"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        out = torch.zeros((batch_size, self.num_classes, self.num_tasks), dtype=x.dtype, device=x.device)\n",
        "        for task in range(self.num_tasks):\n",
        "            classifier = getattr(self, \"fc_\" + str(task))\n",
        "            out[:, :, task] = classifier(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "TypeSkipConnections = Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "\n",
        "\n",
        "class ResNetHIML(ResNet):\n",
        "    \"\"\"Wrapper class of the original torchvision ResNet model.\n",
        "\n",
        "    The forward function is updated to return the penultimate layer\n",
        "    activations, which are required to obtain image patch embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs: Any) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.Tensor, return_intermediate_layers: bool = False\n",
        "    ) -> Union[torch.Tensor, TypeSkipConnections]:\n",
        "        \"\"\"ResNetHIML forward pass. Optionally returns intermediate layers using the\n",
        "        ``return_intermediate_layers`` argument.\n",
        "\n",
        "        :param return_intermediate_layers: If ``True``, return layers x0-x4 as a tuple,\n",
        "            otherwise return x4 only.\n",
        "        \"\"\"\n",
        "\n",
        "        x0 = self.conv1(x)\n",
        "        x0 = self.bn1(x0)\n",
        "        x0 = self.relu(x0)\n",
        "        x0 = self.maxpool(x0)\n",
        "\n",
        "        x1 = self.layer1(x0)\n",
        "        x2 = self.layer2(x1)\n",
        "        x3 = self.layer3(x2)\n",
        "        x4 = self.layer4(x3)\n",
        "\n",
        "        if return_intermediate_layers:\n",
        "            return x0, x1, x2, x3, x4\n",
        "        else:\n",
        "            return x4\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> ResNetHIML:\n",
        "    \"\"\"Instantiate a custom :class:`ResNet` model.\n",
        "\n",
        "    Adapted from :mod:`torchvision.models.resnet`.\n",
        "    \"\"\"\n",
        "    model = ResNetHIML(block=block, layers=layers, **kwargs)\n",
        "    if pretrained:\n",
        "        raise NotImplementedError\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNetHIML:\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    :param pretrained: If ``True``, returns a model pre-trained on ImageNet.\n",
        "    :param progress: If ``True``, displays a progress bar of the download to ``stderr``.\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNetHIML:\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
        "\n",
        "    :param pretrained: If ``True``, returns a model pre-trained on ImageNet\n",
        "    :param progress: If ``True``, displays a progress bar of the download to ``stderr``.\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MultiHeadAttentionOutput:\n",
        "    mha_output: torch.Tensor\n",
        "    attention: Optional[torch.Tensor] = None\n",
        "\n",
        "\n",
        "class VisionTransformerPooler(nn.Module):\n",
        "    \"\"\"\n",
        "    :param input_dim: Input feature dimension (i.e., channels in old CNN terminology)\n",
        "    :param grid_shape: Shape of the grid of patches per image\n",
        "    :param num_heads: Number of self-attention heads within the MHA block\n",
        "    :param num_blocks: Number of blocks per attention layer\n",
        "    :param norm_layer: Normalisation layer\n",
        "\n",
        "    `self.type_embed`: Is used to characterise prior and current scans, and\n",
        "                       create permutation variance across modalities/series.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        grid_shape: Tuple[int, int],\n",
        "        num_heads: int = 8,\n",
        "        num_blocks: int = 3,\n",
        "        norm_layer: Any = partial(nn.LayerNorm, eps=1e-6),\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        block_kwargs = dict(\n",
        "            dim=input_dim,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=1.0,\n",
        "            drop=0.10,\n",
        "            attn_drop=0.10,\n",
        "            drop_path=0.25,\n",
        "            act_layer=nn.GELU,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "        self.blocks = nn.ModuleList([Block(**block_kwargs) for _ in range(num_blocks)])\n",
        "        self.norm_post = norm_layer(input_dim)\n",
        "        self.grid_shape = grid_shape\n",
        "        self.num_patches = grid_shape[0] * grid_shape[1]\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        # Temporal positional embeddings\n",
        "        num_series: int = 2\n",
        "        self.type_embed = nn.Parameter(torch.zeros(num_series, 1, input_dim))\n",
        "        trunc_normal_(self.type_embed, std=0.02)\n",
        "\n",
        "        # Positional embeddings 1 x L x C (L: Sequence length, C: Feature dimension)\n",
        "        self.pos_drop = nn.Dropout(p=0.10)\n",
        "        pos_embed_class = SinePositionEmbedding(embedding_dim=input_dim // 2, normalize=True)\n",
        "        pos_embed = pos_embed_class(mask=torch.ones([1, grid_shape[0], grid_shape[1]]))  # 1 x L x C\n",
        "        self.register_buffer(\"pos_embed\", pos_embed, persistent=False)\n",
        "\n",
        "        # Initialisation\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def no_weight_decay(self) -> Set[str]:\n",
        "        return {'type_embed'}\n",
        "\n",
        "    def forward(self, current_image: torch.Tensor, previous_image: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        B, C, H, W = current_image.shape\n",
        "        assert H == self.grid_shape[0] and W == self.grid_shape[1], \"Input and grid shapes do not match\"\n",
        "\n",
        "        # Flatten patch embeddings to have shape (B x L x C), L = H * W\n",
        "        if previous_image is not None:\n",
        "            assert previous_image.shape == current_image.shape, \"current_image and previous_image shapes do not match\"\n",
        "            previous_image = previous_image.view(B, C, H * W).transpose(1, 2)\n",
        "        current_image = current_image.view(B, C, H * W).transpose(1, 2)\n",
        "        pos_embed = self.pos_embed.repeat(B, 1, 1)  # type: ignore\n",
        "\n",
        "        # Final token activations (B x 2L x C)\n",
        "        token_features = self.forward_after_reshape(x=current_image, pos_embed=pos_embed, x_previous=previous_image)\n",
        "\n",
        "        # Extract the patch features of current image\n",
        "        cur_img_token_id = 0\n",
        "        current_token_features = token_features[:, cur_img_token_id : self.num_patches + cur_img_token_id]\n",
        "        current_patch_features = current_token_features.transpose(1, 2).view(B, C, H, W)\n",
        "\n",
        "        return current_patch_features\n",
        "\n",
        "    def forward_after_reshape(\n",
        "        self, x: torch.Tensor, pos_embed: torch.Tensor, x_previous: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        B, L, _ = x.shape  # Batch, Sequence length, Feature dimension\n",
        "\n",
        "        # Positional and type embeddings\n",
        "        type_embed = self.type_embed[0].expand(B, L, -1)\n",
        "        if x_previous is not None:\n",
        "            x = torch.cat((x, x_previous), dim=1)\n",
        "            pos_embed = torch.cat((pos_embed, pos_embed), dim=1)\n",
        "            prev_type_embed = self.type_embed[1].expand(B, L, -1)\n",
        "            type_embed = torch.cat((type_embed, prev_type_embed), dim=1)\n",
        "\n",
        "        # Add positional and type embeddings (used in query and key matching)\n",
        "        pos_and_type_embed = pos_embed + type_embed\n",
        "\n",
        "        # Positional dropout\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Multihead attention followed by MLP\n",
        "        for block in self.blocks:\n",
        "            x = block(x=x, pos_and_type_embed=pos_and_type_embed)\n",
        "        x = self.norm_post(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_weights(self, m: nn.Module) -> None:\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self attention module\n",
        "\n",
        "    The content builds on top of the TIMM library (vision_transformer.py) and differs by the following:\n",
        "        - Defines a custom `MultiHeadAttentionLayer` which does not only apply `self-attention` but it can be\n",
        "            generalised to arbitrary (query, key, value) input tuples. This feature can be valuable to process\n",
        "            more than 2 scans at a time.\n",
        "        - `Self-attention` specific use-case can still be invoked by calling the `forward_as_mhsa` method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dim: int, num_heads: int = 8, qkv_bias: bool = False, attn_drop: float = 0.0, proj_drop: float = 0.0\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        assert dim % num_heads == 0, f\"The embedding dim ({dim}) must be divisible by the number of heads ({num_heads})\"\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim**-0.5\n",
        "        self.return_attention = False\n",
        "\n",
        "        self.proj_q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.proj_k = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "        self.proj_v = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, k: torch.Tensor, q: torch.Tensor, v: torch.Tensor) -> MultiHeadAttentionOutput:\n",
        "        B, N, C = v.shape\n",
        "        assert (\n",
        "            C % self.num_heads == 0\n",
        "        ), f\"The embedding dim ({C}) must be divisible by the number of heads ({self.num_heads})\"\n",
        "\n",
        "        w_q = self.proj_q(q).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        w_k = self.proj_k(k).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        w_v = self.proj_v(v).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (w_q @ w_k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        o = (attn @ w_v).transpose(1, 2).reshape(B, N, C)\n",
        "        o = self.proj(o)\n",
        "        o = self.proj_drop(o)\n",
        "\n",
        "        attention_output = attn if self.return_attention else None\n",
        "\n",
        "        return MultiHeadAttentionOutput(mha_output=o, attention=attention_output)\n",
        "\n",
        "    def forward_as_mhsa(self, input: torch.Tensor) -> MultiHeadAttentionOutput:\n",
        "        return self(k=input, q=input, v=input)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Encapsulates multi-layer perceptron and multi-head self attention modules into a block.\n",
        "\n",
        "    The content builds on top of the TIMM library (vision_transformer.py) and differs by the following:\n",
        "        - This implementation uses spatio-temporal positional embeddings instead of 2D positional embeddings only,\n",
        "            and they are taken into account within the forward pass of each ViT block.\n",
        "        - Utilises the custom defined `MultiHeadAttentionLayer` which does not apply `self-attention` only but can be\n",
        "            generalised to arbitrary (query, key, value) tuples. This can be valuable to process more than 2 scans.\n",
        "\n",
        "    Positional and type embeddings are handled in a similar fashion as DETR object localisation paper\n",
        "    https://alcinos.github.io/detr_page/, where a fixed set of sine/cos positional embeddings are used\n",
        "    in an additive manner to Q and K tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_ratio: float = 1.0,\n",
        "        qkv_bias: bool = False,\n",
        "        drop: float = 0.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        drop_path: float = 0.0,\n",
        "        act_layer: Callable = nn.GELU,\n",
        "        norm_layer: Callable = nn.LayerNorm,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = MultiHeadAttentionLayer(\n",
        "            dim=dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop\n",
        "        )\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def with_pos_and_type_embed(self, tensor: torch.Tensor, emb: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        # Add positional embeddings to key and query tensors\n",
        "        return tensor if emb is None else tensor + emb\n",
        "\n",
        "    def forward(self, x: torch.Tensor, pos_and_type_embed: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        x_with_emb = self.with_pos_and_type_embed(self.norm1(x), emb=pos_and_type_embed)\n",
        "        x = x + self.drop_path(self.attn.forward_as_mhsa(x_with_emb).mha_output)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SinePositionEmbedding:\n",
        "    \"\"\"\n",
        "    This is a more standard version of the position embedding, very similar to the one used by the Attention is all you\n",
        "    need paper, generalized to work on images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, embedding_dim: int = 64, temperature: int = 10000, normalize: bool = False, scale: Optional[float] = None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def __call__(self, mask: torch.Tensor) -> torch.Tensor:\n",
        "        assert mask is not None, \"No pixel mask provided\"\n",
        "        B, H, W = mask.shape\n",
        "        y_embed = mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + 1e-6) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + 1e-6) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.embedding_dim, dtype=torch.float32)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.embedding_dim)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).view(B, H * W, self.embedding_dim * 2)\n",
        "\n",
        "        return pos\n",
        "\n",
        "\n",
        "DEFAULT_DILATION_VALUES_FOR_RESNET = (False, False, True)\n",
        "ImageEncoderOutputType = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    \"\"\"Image encoder trunk module for the ``ImageModel`` class.\n",
        "\n",
        "    :param img_encoder_type : Type of image encoder model to use, either ``\"resnet18_multi_image\"`` or\n",
        "                              ``\"resnet50_multi_image\"``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_encoder_type: str):\n",
        "        super().__init__()\n",
        "        self.img_encoder_type = img_encoder_type\n",
        "        self.encoder = self._create_encoder()\n",
        "\n",
        "    def _create_encoder(self, **kwargs: Any) -> nn.Module:\n",
        "        if self.img_encoder_type in [ImageEncoderType.RESNET18, ImageEncoderType.RESNET18_MULTI_IMAGE]:\n",
        "            encoder_class = resnet18\n",
        "        elif self.img_encoder_type in [ImageEncoderType.RESNET50, ImageEncoderType.RESNET50_MULTI_IMAGE]:\n",
        "            encoder_class = resnet50\n",
        "        else:\n",
        "            supported = ImageEncoderType.get_members(multi_image_encoders_only=False)\n",
        "            raise NotImplementedError(f\"Image encoder type \\\"{self.img_encoder_type}\\\" must be in {supported}\")\n",
        "\n",
        "        encoder = encoder_class(pretrained=False, **kwargs)\n",
        "\n",
        "        return encoder\n",
        "\n",
        "    def forward(self, current_image: torch.Tensor, return_patch_embeddings: bool = False) -> ImageEncoderOutputType:\n",
        "        \"\"\"Get image global and patch embeddings\"\"\"\n",
        "\n",
        "        patch_emb = self.encoder(current_image)\n",
        "        avg_pooled_emb = torch.flatten(torch.nn.functional.adaptive_avg_pool2d(patch_emb, (1, 1)), 1)\n",
        "        if return_patch_embeddings:\n",
        "            return patch_emb, avg_pooled_emb\n",
        "\n",
        "        return avg_pooled_emb\n",
        "\n",
        "    def reload_encoder_with_dilation(self, replace_stride_with_dilation: Optional[Sequence[bool]] = None) -> None:\n",
        "        \"\"\"Workaround for enabling dilated convolutions after model initialization.\n",
        "\n",
        "        :param replace_stride_with_dilation: Replace the 2x2 standard convolution stride with a dilated convolution\n",
        "                                             in each layer in the last three blocks of ResNet architecture.\n",
        "        \"\"\"\n",
        "        if self.img_encoder_type == ImageEncoderType.RESNET18:\n",
        "            # resnet18 uses BasicBlock implementation, which does not support dilated convolutions.\n",
        "            raise NotImplementedError(\"resnet18 does not support dilated convolutions\")\n",
        "\n",
        "        if replace_stride_with_dilation is None:\n",
        "            replace_stride_with_dilation = DEFAULT_DILATION_VALUES_FOR_RESNET\n",
        "\n",
        "        device = next(self.encoder.parameters()).device\n",
        "        new_encoder = self._create_encoder(replace_stride_with_dilation=replace_stride_with_dilation).to(device)\n",
        "\n",
        "        if self.encoder.training:\n",
        "            new_encoder.train()\n",
        "        else:\n",
        "            new_encoder.eval()\n",
        "\n",
        "        new_encoder.load_state_dict(self.encoder.state_dict())\n",
        "        self.encoder = new_encoder\n",
        "\n",
        "\n",
        "class MultiImageEncoder(ImageEncoder):\n",
        "    \"\"\"Multi-image encoder trunk module for the ``ImageModel`` class.\n",
        "    It can be used to encode multiple images into combined latent representation.\n",
        "    Currently it only supports two input images but can be extended to support more in future.\n",
        "\n",
        "    :param img_encoder_type: Type of image encoder model to use: either ``\"resnet18\"`` or ``\"resnet50\"``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_encoder_type: str):\n",
        "        super().__init__(img_encoder_type)\n",
        "\n",
        "        output_dim = 256  # The aggregate feature dim of the encoder is `2 * output_dim` i.e. [f_static, f_diff]\n",
        "        grid_shape = (14, 14)  # Spatial dimensions of patch grid.\n",
        "\n",
        "        backbone_output_feature_dim = get_encoder_output_dim(self.encoder, device=get_module_device(self))\n",
        "\n",
        "        self.backbone_to_vit = nn.Conv2d(\n",
        "            in_channels=backbone_output_feature_dim,\n",
        "            out_channels=output_dim,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.vit_pooler = VisionTransformerPooler(input_dim=output_dim, grid_shape=grid_shape)\n",
        "\n",
        "        # Missing image embedding\n",
        "        self.missing_previous_emb = nn.Parameter(torch.zeros(1, output_dim, 1, 1))\n",
        "        trunc_normal_(self.missing_previous_emb, std=0.02)\n",
        "\n",
        "    def forward(  # type: ignore[override]\n",
        "        self,\n",
        "        current_image: torch.Tensor,\n",
        "        previous_image: Optional[torch.Tensor] = None,\n",
        "        return_patch_embeddings: bool = False,\n",
        "    ) -> ImageEncoderOutputType:\n",
        "        batch_size = current_image.shape[0]\n",
        "\n",
        "        if previous_image is not None:\n",
        "            assert current_image.shape == previous_image.shape\n",
        "            x = torch.cat([current_image, previous_image], dim=0)\n",
        "            x = super().forward(x, return_patch_embeddings=True)[0]\n",
        "            x = self.backbone_to_vit(x)\n",
        "            patch_x, patch_x_previous = x[:batch_size], x[batch_size:]\n",
        "            diff_x = self.vit_pooler(current_image=patch_x, previous_image=patch_x_previous)\n",
        "        else:\n",
        "            x = super().forward(current_image, return_patch_embeddings=True)[0]\n",
        "            patch_x = self.backbone_to_vit(x)\n",
        "            B, _, W, H = patch_x.shape\n",
        "            diff_x = self.missing_previous_emb.repeat(B, 1, W, H)\n",
        "\n",
        "        patch_fused = torch.cat([patch_x, diff_x], dim=1)\n",
        "        avg_pooled_emb = torch.flatten(torch.nn.functional.adaptive_avg_pool2d(patch_fused, (1, 1)), 1)\n",
        "\n",
        "        if return_patch_embeddings:\n",
        "            return patch_fused, avg_pooled_emb\n",
        "\n",
        "        return avg_pooled_emb\n",
        "\n",
        "    def reload_encoder_with_dilation(self, replace_stride_with_dilation: Optional[Sequence[bool]] = None) -> None:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_encoder_output_dim(module: torch.nn.Module, device: torch.device) -> int:\n",
        "    \"\"\"Calculate the output dimension of an encoder by making a single forward pass.\n",
        "\n",
        "    :param module: Encoder module.\n",
        "    :param device: Compute device to use.\n",
        "    \"\"\"\n",
        "    # Target device\n",
        "    assert isinstance(device, torch.device)\n",
        "\n",
        "    x = torch.rand((1, 3, 448, 448)).to(device)\n",
        "\n",
        "    # Extract the number of output feature dimensions\n",
        "    with restore_training_mode(module):\n",
        "        module.eval()\n",
        "        representations = module(x)\n",
        "    return representations.shape[1]\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def restore_training_mode(module: nn.Module) -> Generator[None, None, None]:\n",
        "    \"\"\"Restore the training mode of a module after some operation.\n",
        "\n",
        "    :param module: PyTorch module.\n",
        "    \"\"\"\n",
        "    training_mode = module.training\n",
        "    yield\n",
        "    module.train(mode=training_mode)\n",
        "\n",
        "\n",
        "def get_encoder_from_type(img_encoder_type: str) -> ImageEncoder:\n",
        "    \"\"\"Returns the encoder class for the given encoder type.\n",
        "\n",
        "    :param img_encoder_type: Encoder type. {RESNET18, RESNET50, RESNET18_MULTI_IMAGE, RESNET50_MULTI_IMAGE}\n",
        "    \"\"\"\n",
        "    if img_encoder_type in ImageEncoderType.get_members(multi_image_encoders_only=True):\n",
        "        return MultiImageEncoder(img_encoder_type=img_encoder_type)\n",
        "    else:\n",
        "        return ImageEncoder(img_encoder_type=img_encoder_type)\n",
        "\n",
        "\n",
        "class BaseImageModel(nn.Module, ABC):\n",
        "    \"\"\"Abstract class for image models.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, *args: Any, **kwargs: Any) -> ImageModelOutput:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_patchwise_projected_embeddings(self, input_img: torch.Tensor, normalize: bool) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class ImageModel(BaseImageModel):\n",
        "    \"\"\"Image encoder module\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_encoder_type: str,\n",
        "        joint_feature_size: int,\n",
        "        freeze_encoder: bool = False,\n",
        "        pretrained_model_path: Optional[Union[str, Path]] = None,\n",
        "        **downstream_classifier_kwargs: Any,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initiate encoder, projector, and classifier\n",
        "        self.encoder = get_encoder_from_type(img_encoder_type)\n",
        "        self.feature_size = get_encoder_output_dim(self.encoder, device=get_module_device(self.encoder))\n",
        "        self.projector = MLP(\n",
        "            input_dim=self.feature_size,\n",
        "            output_dim=joint_feature_size,\n",
        "            hidden_dim=joint_feature_size,\n",
        "            use_1x1_convs=True,\n",
        "        )\n",
        "        self.downstream_classifier_kwargs = downstream_classifier_kwargs\n",
        "        self.classifier = self.create_downstream_classifier() if downstream_classifier_kwargs else None\n",
        "\n",
        "        # Initialise the mode of modules\n",
        "        self.freeze_encoder = freeze_encoder\n",
        "        self.train()\n",
        "\n",
        "        if pretrained_model_path is not None:\n",
        "            if not isinstance(pretrained_model_path, (str, Path)):\n",
        "                raise TypeError(f\"Expected a string or Path, got {type(pretrained_model_path)}\")\n",
        "            state_dict = torch.load(pretrained_model_path, map_location=\"cpu\")\n",
        "            self.load_state_dict(state_dict)\n",
        "\n",
        "    def train(self, mode: bool = True) -> Any:\n",
        "        \"\"\"Switch the model between training and evaluation modes.\"\"\"\n",
        "        super().train(mode=mode)\n",
        "        if self.freeze_encoder:\n",
        "            self.encoder.train(mode=False)\n",
        "            self.projector.train(mode=False)\n",
        "        return self\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> ImageModelOutput:  # type: ignore[override]\n",
        "        with torch.set_grad_enabled(not self.freeze_encoder):\n",
        "            patch_x, pooled_x = self.encoder(x, return_patch_embeddings=True)\n",
        "        return self.forward_post_encoder(patch_x, pooled_x)\n",
        "\n",
        "    def forward_post_encoder(self, patch_x: torch.Tensor, pooled_x: torch.Tensor) -> ImageModelOutput:\n",
        "        with torch.set_grad_enabled(not self.freeze_encoder):\n",
        "            projected_patch_embeddings = self.projector(patch_x)\n",
        "            projected_global_embedding = torch.mean(projected_patch_embeddings, dim=(2, 3))\n",
        "\n",
        "        logits = self.classifier(pooled_x) if self.classifier else None\n",
        "        return ImageModelOutput(\n",
        "            img_embedding=pooled_x,\n",
        "            patch_embeddings=patch_x,\n",
        "            class_logits=logits,\n",
        "            projected_patch_embeddings=projected_patch_embeddings,\n",
        "            projected_global_embedding=projected_global_embedding,\n",
        "        )\n",
        "\n",
        "    def create_downstream_classifier(self, **kwargs: Any) -> MultiTaskModel:\n",
        "        \"\"\"Create the classification module for the downstream task.\"\"\"\n",
        "        downstream_classifier_kwargs = kwargs if kwargs else self.downstream_classifier_kwargs\n",
        "        return MultiTaskModel(self.feature_size, **downstream_classifier_kwargs)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_patchwise_projected_embeddings(self, input_img: torch.Tensor, normalize: bool) -> torch.Tensor:\n",
        "        \"\"\"Get patch-wise projected embeddings from the CNN model.\n",
        "\n",
        "        :param input_img: input tensor image [B, C, H, W].\n",
        "        :param normalize: If ``True``, the embeddings are L2-normalized.\n",
        "        :returns projected_embeddings: tensor of embeddings in shape [batch, n_patches_h, n_patches_w, feature_size].\n",
        "        \"\"\"\n",
        "        assert not self.training, \"This function is only implemented for evaluation mode\"\n",
        "        outputs = self.forward(input_img)\n",
        "        projected_embeddings = outputs.projected_patch_embeddings.detach()  # type: ignore\n",
        "        if normalize:\n",
        "            projected_embeddings = F.normalize(projected_embeddings, dim=1)\n",
        "        projected_embeddings = projected_embeddings.permute([0, 2, 3, 1])  # B D H W -> B H W D (D: Features)\n",
        "        return projected_embeddings\n",
        "\n",
        "\n",
        "class MultiImageModel(ImageModel):\n",
        "    def __init__(self, **kwargs: Any) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        assert isinstance(self.encoder, MultiImageEncoder), \"MultiImageModel only supports MultiImageEncoder\"\n",
        "\n",
        "    def forward(  # type: ignore[override]\n",
        "        self, current_image: torch.Tensor, previous_image: Optional[torch.Tensor] = None\n",
        "    ) -> ImageModelOutput:\n",
        "        with torch.set_grad_enabled(not self.freeze_encoder):\n",
        "            patch_x, pooled_x = self.encoder(\n",
        "                current_image=current_image, previous_image=previous_image, return_patch_embeddings=True\n",
        "            )\n",
        "        return self.forward_post_encoder(patch_x, pooled_x)\n",
        "\n",
        "\n",
        "\n",
        "JOINT_FEATURE_SIZE = 128\n",
        "\n",
        "BIOMED_VLP_CXR_BERT_SPECIALIZED = \"microsoft/BiomedVLP-CXR-BERT-specialized\"\n",
        "BIOMED_VLP_BIOVIL_T = \"microsoft/BiomedVLP-BioViL-T\"\n",
        "HF_URL = \"https://huggingface.co\"\n",
        "\n",
        "CXR_BERT_COMMIT_TAG = \"v1.1\"\n",
        "BIOVIL_T_COMMIT_TAG = \"v1.0\"\n",
        "\n",
        "BIOVIL_IMAGE_WEIGHTS_NAME = \"biovil_image_resnet50_proj_size_128.pt\"\n",
        "BIOVIL_IMAGE_WEIGHTS_URL = f\"{HF_URL}/{BIOMED_VLP_CXR_BERT_SPECIALIZED}/resolve/{CXR_BERT_COMMIT_TAG}/{BIOVIL_IMAGE_WEIGHTS_NAME}\"  # noqa: E501\n",
        "BIOVIL_IMAGE_WEIGHTS_MD5 = \"02ce6ee460f72efd599295f440dbb453\"\n",
        "\n",
        "BIOVIL_T_IMAGE_WEIGHTS_NAME = \"biovil_t_image_model_proj_size_128.pt\"\n",
        "BIOVIL_T_IMAGE_WEIGHTS_URL = (\n",
        "    f\"{HF_URL}/{BIOMED_VLP_BIOVIL_T}/resolve/{BIOVIL_T_COMMIT_TAG}/{BIOVIL_T_IMAGE_WEIGHTS_NAME}\"  # noqa: E501\n",
        ")\n",
        "BIOVIL_T_IMAGE_WEIGHTS_MD5 = \"a83080e2f23aa584a4f2b24c39b1bb64\"\n",
        "\n",
        "\n",
        "def _download_biovil_image_model_weights() -> Path:\n",
        "    \"\"\"Download image model weights from Hugging Face.\n",
        "\n",
        "    More information available at https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized.\n",
        "    \"\"\"\n",
        "    root_dir = tempfile.gettempdir()\n",
        "    download_url(\n",
        "        BIOVIL_IMAGE_WEIGHTS_URL,\n",
        "        root=root_dir,\n",
        "        filename=BIOVIL_IMAGE_WEIGHTS_NAME,\n",
        "        md5=BIOVIL_IMAGE_WEIGHTS_MD5,\n",
        "    )\n",
        "    return Path(root_dir, BIOVIL_IMAGE_WEIGHTS_NAME)\n",
        "\n",
        "\n",
        "def _download_biovil_t_image_model_weights() -> Path:\n",
        "    \"\"\"Download image model weights from Hugging Face.\n",
        "\n",
        "    More information available at https://huggingface.co/microsoft/microsoft/BiomedVLP-BioViL-T.\n",
        "    \"\"\"\n",
        "    root_dir = tempfile.gettempdir()\n",
        "    download_url(\n",
        "        BIOVIL_T_IMAGE_WEIGHTS_URL, root=root_dir, filename=BIOVIL_T_IMAGE_WEIGHTS_NAME, md5=BIOVIL_T_IMAGE_WEIGHTS_MD5\n",
        "    )\n",
        "    return Path(root_dir, BIOVIL_T_IMAGE_WEIGHTS_NAME)\n",
        "\n",
        "\n",
        "def get_biovil_image_encoder(pretrained: bool = True) -> ImageModel:\n",
        "    \"\"\"Download weights from Hugging Face and instantiate the image model.\"\"\"\n",
        "    resnet_checkpoint_path = _download_biovil_image_model_weights() if pretrained else None\n",
        "\n",
        "    image_model = ImageModel(\n",
        "        img_encoder_type=ImageEncoderType.RESNET50,\n",
        "        joint_feature_size=JOINT_FEATURE_SIZE,\n",
        "        pretrained_model_path=resnet_checkpoint_path,\n",
        "    )\n",
        "    return image_model\n",
        "\n",
        "\n",
        "def get_biovil_t_image_encoder() -> ImageModel:\n",
        "    \"\"\"Download weights from Hugging Face and instantiate the image model.\"\"\"\n",
        "\n",
        "    biovilt_checkpoint_path = _download_biovil_t_image_model_weights()\n",
        "    model_type = ImageEncoderType.RESNET50_MULTI_IMAGE\n",
        "    image_model = ImageModel(\n",
        "        img_encoder_type=model_type,\n",
        "        joint_feature_size=JOINT_FEATURE_SIZE,\n",
        "        pretrained_model_path=biovilt_checkpoint_path,\n",
        "    )\n",
        "    return image_model\n",
        "\n",
        "\n",
        "def get_imagenet_init_encoder() -> ImageModel:\n",
        "    \"\"\"Download ImageNet pre-trained weights and instantiate the image model.\"\"\"\n",
        "    raise NotImplemented\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model and move to GPU"
      ],
      "metadata": {
        "id": "lSLSf3AJCTzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUVarGmpiXy7"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Resize, CenterCrop, ToTensor, Compose\n",
        "import torch\n",
        "\n",
        "# wrapper for image encoder of BioVil for convenience\n",
        "class ImageEncoderBioVil(nn.Module):\n",
        "    \"\"\"Instantiate image model with pre-trained weights.\n",
        "    :param weights: Select one of `biovil`, `biovil_t`\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone='biovil'):\n",
        "        super().__init__()\n",
        "        if backbone == ImageEncoderWeightTypes.BIOVIL:\n",
        "            self.backbone = get_biovil_image_encoder()\n",
        "        elif backbone == ImageEncoderWeightTypes.BIOVIL_T:\n",
        "            self.backbone = get_biovil_t_image_encoder()\n",
        "        else:\n",
        "            raise ValueError(f\"Weights option not found: {backbone}\")\n",
        "\n",
        "    def forward(self, image):\n",
        "        return self.backbone(image)\n",
        "\n",
        "# image transformations used by BioVil\n",
        "image_transforms = Compose([Resize(512, antialias=True), CenterCrop(512), ToTensor()])\n",
        "\n",
        "# load image encoder\n",
        "image_encoder = ImageEncoderBioVil('biovil_t')\n",
        "\n",
        "# set device for models - ideally using a GPU for fast performance (=cuda)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def print_red(text):\n",
        "    print('\\033[91m' + text + '\\033[0m')\n",
        "\n",
        "def print_green(text):\n",
        "    print('\\033[92m' + text + '\\033[0m')\n",
        "\n",
        "if device == 'cpu':\n",
        "    print_red('No GPU accelerator found, falling back to CPU - model inference will be slow')\n",
        "else:\n",
        "    print_green('Running on GPU! (fast inference)')\n",
        "\n",
        "image_encoder.eval()\n",
        "image_encoder.to(device)\n",
        "\n",
        "# helper method for getting image embedding from BioVil\n",
        "def get_image_embeddings(images):\n",
        "    with torch.no_grad():\n",
        "        transformed_images = torch.stack([image_transforms(image) for image in images])\n",
        "        image_model_output = image_encoder(transformed_images.to(device))\n",
        "        image_embeddings = image_model_output.projected_global_embedding\n",
        "        return image_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BjvezaDptku"
      },
      "source": [
        "## Loading Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34sFB7Xg-VZh"
      },
      "outputs": [],
      "source": [
        "#@title BioVil BERT\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "url = \"microsoft/BiomedVLP-BioViL-T\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(url, trust_remote_code=True)\n",
        "text_encoder = AutoModel.from_pretrained(url, trust_remote_code=True)\n",
        "text_encoder.eval()\n",
        "text_encoder.to(device)\n",
        "\n",
        "# Tokenize and compute the sentence embeddings\n",
        "def get_text_embeddings(text_prompts):\n",
        "    with torch.no_grad():\n",
        "        tokenizer_output = tokenizer.batch_encode_plus(\n",
        "            batch_text_or_text_pairs=text_prompts,\n",
        "            add_special_tokens=True,\n",
        "            padding='longest',\n",
        "            return_tensors='pt',\n",
        "        ).to(device)\n",
        "        text_embeddings = text_encoder(\n",
        "            input_ids=tokenizer_output.input_ids,\n",
        "            attention_mask=tokenizer_output.attention_mask,\n",
        "            output_cls_projected_embedding=True,\n",
        "        )\n",
        "        return text_embeddings.cls_projected_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abfNgnAGsK9K"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsrm5n2xsQcx"
      },
      "source": [
        "### Cosine Similarity\n",
        "The cosine similarity is a similarity measure between two vectors that only compares the similarity of their direction, not their length. Mathematically the dot product is calculated between the normalized vectors.\n",
        "\n",
        "More information on wikipedia: https://en.wikipedia.org/wiki/Cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y659P1ZDklvZ"
      },
      "outputs": [],
      "source": [
        "def calculate_cosine_similarity(embedding_1, embedding_2):\n",
        "    # Normalize the vectors to a length of one and then perform a matrix multiplication (dot product)\n",
        "    return F.normalize(embedding_1) @ F.normalize(embedding_2).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KApNp1yPBwLE"
      },
      "source": [
        "### Download image\n",
        "Helper method to download an image from a given url."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96ExxQppB2Jy"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(url, cache_directory='images'):\n",
        "    # get file name of chest x-ray\n",
        "    parsed_url = urlparse(url)\n",
        "    image_name = Path(parsed_url.path).name\n",
        "    # create 'images' folder as cache directory in case it does not exist yet\n",
        "    cache_path = Path(cache_directory)\n",
        "    cache_path.mkdir(exist_ok=True)\n",
        "    # define local image path\n",
        "    image_path = cache_path / image_name\n",
        "    # if the image is not there yet download it\n",
        "    if not image_path.exists():\n",
        "        with urllib.request.urlopen(url) as response, image_path.open('wb') as image_file:\n",
        "            image_file.write(response.read())\n",
        "    # open image with the Pillow library and return it\n",
        "    return Image.open(image_path)\n",
        "\n",
        "# test with a random sample from the Indiana chest x-ray dataset\n",
        "url = 'https://openi.nlm.nih.gov/imgs/512/276/677/CXR677_IM-2249-1001.png?keywords=Catheters,%20Indwelling,Lung,Density,Density,Density,Pleural%20Effusion,Pneumonia'\n",
        "print('downloaded url:', url)\n",
        "display(load_image(url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KZTmu3p2jD4"
      },
      "source": [
        "### Plot results\n",
        "This is another helper method to visualize the cosine similarities between text and images as well as softmax probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SOgbmhjr2vHS"
      },
      "outputs": [],
      "source": [
        "#@title Visualization method for text-image cosine similarity\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox, AnchoredOffsetbox\n",
        "\n",
        "# Creating a dataframe for seaborn heatmap\n",
        "def plot_similarities(similarities, text_prompts, images, image_descriptions, plot_probabilities=False, size_unit = 1):\n",
        "    if plot_probabilities:\n",
        "        title = 'Softmax probabilities'\n",
        "        # apply softmax to similarities for each image\n",
        "        probabilities = similarities.softmax(dim=-1)\n",
        "        matrix = probabilities\n",
        "        # define range of probabilities 0%-100%\n",
        "        vmin = 0\n",
        "        vmax = 1\n",
        "        # formatting\n",
        "        fmt = '.0%'\n",
        "    else:\n",
        "        title = 'Cosine similarities'\n",
        "        matrix = similarities\n",
        "        # define range of cosine similarity\n",
        "        vmin = -1\n",
        "        vmax = 1\n",
        "        # format\n",
        "        fmt =  '.2f'\n",
        "    for prompt in text_prompts:\n",
        "        if len(prompt)>40:\n",
        "            prompt = prompt.replace('. ', '.\\n')\n",
        "    # create table with matrix values and column / row names\n",
        "    df = pd.DataFrame(matrix.T.cpu().detach().numpy(), columns=image_descriptions, index=text_prompts)\n",
        "    # adjust fig size to number of images and prompts\n",
        "    fig, ax = plt.subplots(figsize=(size_unit+(size_unit*len(images)),size_unit+size_unit*len(text_prompts)))\n",
        "    # plot heatmap\n",
        "    heatmap = sns.heatmap(df, annot=True, cbar=False, cmap='RdYlGn', vmin=0, vmax=1, linewidths=0.5, linecolor='black', square=True, annot_kws={\"fontsize\": 12}, fmt=fmt)\n",
        "    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0)\n",
        "\n",
        "    # Set labels\n",
        "    heatmap.set_xlabel(\"Patient images\", weight='bold')  # x-axis label\n",
        "    heatmap.set_ylabel(\"Text prompts\", weight='bold')  # y-axis label\n",
        "\n",
        "    # Calculate the positions of the cells for placing images\n",
        "    y_positions = ax.get_yticks()\n",
        "    x_positions = ax.get_xticks()\n",
        "\n",
        "    # Add images to the top of each column\n",
        "    for i, img in enumerate(images):\n",
        "\n",
        "        # Make the image the same size as a cell\n",
        "        imagebox = OffsetImage(img, zoom=0.11 * size_unit)\n",
        "\n",
        "        # Place the image at the top of the column\n",
        "        ab = AnnotationBbox(imagebox, (x_positions[i], 0), box_alignment=(0.5, -0.05), frameon=False)\n",
        "        ax.add_artist(ab)\n",
        "    # plt.title(title, fontsize=16, weight='bold')\n",
        "    # fig.suptitle(title, fontsize=16, weight='bold')\n",
        "    fig.text(0.5, 1.2, title, fontsize=12, weight='bold', ha='center')  # Add a title to the figure\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yZotuYksrBR"
      },
      "source": [
        "# Downstream Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIJvIr5OATU"
      },
      "source": [
        "## Test patients - Indiana University, Chest X-rays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDGc74WeTlWW"
      },
      "source": [
        "Patient class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT3yWKoROC0X"
      },
      "outputs": [],
      "source": [
        "class Patient():\n",
        "    def __init__(self, image_url, report):\n",
        "        # store image\n",
        "        self.image = load_image(image_url)\n",
        "        parsed_url = urlparse(image_url)\n",
        "        # patient identifier\n",
        "        self.patient_id = Path(parsed_url.path).stem\n",
        "        # extract keywords from url\n",
        "        self.keywords = parsed_url.query.split('=')[1].replace('%20', ' ')\n",
        "        self.url = 'https://openi.nlm.nih.gov/detailedresult?img=' + self.patient_id\n",
        "        # A radiology report consists of different sections, including indication, findings and impression\n",
        "        # For the purpose of this exercise, we only look at findings that provide the most detailed account of the image\n",
        "        self.report = report\n",
        "\n",
        "    def __str__(self):\n",
        "        # just for convenience to see patient details\n",
        "        return f\"\"\"\n",
        "Patient ID: {self.patient_id}\n",
        "Patient URL: {self.url}\n",
        "\n",
        "RADIOLOGY REPORT\n",
        "-------------------------------------------\n",
        "{self.report}\n",
        "-------------------------------------------\n",
        "\\n\\n\n",
        "\"\"\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        # display the image\n",
        "        display(self.image)\n",
        "        # Call __str__ method\n",
        "        return self.__str__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nFecGppTuHB"
      },
      "source": [
        "### Patient Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1saOi96Tyom"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# dictionary with patients\n",
        "patients = {}\n",
        "\n",
        "patients['healthy'] = Patient(\n",
        "    image_url = 'https://openi.nlm.nih.gov/imgs/512/393/3200/CXR3200_IM-1512-1001.png?keywords=normal',\n",
        "    report = 'Heart size is normal and the lungs are clear.'\n",
        ")\n",
        "\n",
        "patients['pneumonia'] = Patient(\n",
        "    image_url = 'https://openi.nlm.nih.gov/imgs/512/276/677/CXR677_IM-2249-1001.png?keywords=Catheters,%20Indwelling,Lung,Density,Density,Density,Pleural%20Effusion,Pneumonia',\n",
        "    report = 'PICC line catheter tip XXXX in the right atrium. Heart is not enlarged. Trachea and XXXX bronchi appear normal. Lungs are mildly under expanded. No pneumothorax. There are small areas of patchy density in the left lower lung XXXX. There is a larger area of XXXX patchy density in the right mid and lower lungs with right-sided pleural effusion.',\n",
        ")\n",
        "\n",
        "patients['atelectasis'] = Patient(\n",
        "    image_url = 'https://openi.nlm.nih.gov/imgs/512/242/1445/CXR1445_IM-0287-4004.png?keywords=Diaphragm,Pulmonary%20Atelectasis,Consolidation,Pleural%20Effusion,Catheters,%20Indwelling,Tube,%20Inserted,Airspace%20Disease',\n",
        "    report = 'Stable cardiomediastinal silhouette. There has been interval removal of right chest tube with increased elevation of the right hemidiaphragm and XXXX right basilar atelectasis. Left basilar consolidation and pleural effusions seen. No XXXX focal consolidation or pneumothorax. There is a stable left PICC with tip overlying the mid SVC and large XXXX feeding tube courses below the diaphragm.'\n",
        ")\n",
        "\n",
        "patients['cardiomegaly'] = Patient(\n",
        "    image_url = 'https://openi.nlm.nih.gov/imgs/512/309/1111/CXR1111_IM-0077-4004.png?keywords=Technical%20Quality%20of%20Image%20Unsatisfactory%20,Cardiomegaly',\n",
        "    report = 'Lordotic projection and large body habitus. Limited mediastinal evaluation. Severe cardiomegaly. No visualized pneumothorax. No large effusion or airspace disease. No fracture.',\n",
        ")\n",
        "\n",
        "patients['Nodules'] = Patient(\n",
        "    image_url = 'https://openi.nlm.nih.gov/imgs/512/22/1626/CXR1626_IM-0407-1001.png?keywords=Nodule,Nodule',\n",
        "    report = 'The heart is normal in size. The mediastinal contours are within normal limits. There are numerous bilateral pulmonary nodules of varying sizes. The largest is noted in the left lower lobe, posteriorly measuring approximately 7.0 cm. No acute infiltrate or pleural effusion are appreciated.'\n",
        ")\n",
        "\n",
        "for i, (description, patient) in enumerate(patients.items(), 1):\n",
        "    print(f'Patient {i}:', description)\n",
        "    display(patient)\n",
        "\n",
        "patient_images = [patient.image for patient in patients.values()]\n",
        "patient_reports = [patient.report for patient in patients.values()]\n",
        "patient_descriptions = [description for description in patients.keys()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDF7Aod_syiS"
      },
      "source": [
        "## Zero-shot X-Ray Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho5_9585-T4A"
      },
      "source": [
        "### Contrastive binary classification\n",
        "The basic idea of contrastive zero-shot classification is to encode both a positive and negative description (e.g. presence and absence) of a class to be predicted. Next, the image is encoded in the same space and then evaluated if it is closer to the positive or negative text embedding. The [softmax function](https://en.wikipedia.org/wiki/Softmax_function) allows us to estimate a probability for this prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "524elvoCsr5e"
      },
      "outputs": [],
      "source": [
        "def get_similarities_from_text_and_images(text_prompts, images):\n",
        "    text_embeddings = get_text_embeddings(text_prompts)\n",
        "    image_embeddings = get_image_embeddings(images)\n",
        "    similarities = calculate_cosine_similarity(image_embeddings, text_embeddings)\n",
        "    return similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktEL8PO28yRl"
      },
      "source": [
        "#### Basic Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK8JPykMI7xz"
      },
      "source": [
        "##### Healthy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLGcbLsfI-aP"
      },
      "outputs": [],
      "source": [
        "text_prompts = [\n",
        "    'healthy',\n",
        "    'not healthy',\n",
        "]\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9RYug88IyOi"
      },
      "source": [
        "##### Cardiomegaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcPXqMdzALOG"
      },
      "outputs": [],
      "source": [
        "text_prompts = [\n",
        "    'cardiomegaly',\n",
        "    'no cardiomegaly',\n",
        "]\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Pleural effusion"
      ],
      "metadata": {
        "id": "ykvCp2AFgDWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompts = [\n",
        "    'pleural effusion',\n",
        "    'no pleural effusion',\n",
        "]\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ],
      "metadata": {
        "id": "M0WqPfVjft0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TGoeLheIlKV"
      },
      "source": [
        "##### Pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFLezUWa80Pa"
      },
      "outputs": [],
      "source": [
        "text_prompts = [\n",
        "    'pneumonia',\n",
        "    'no pneumonia',\n",
        "]\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF0refoQIpHt"
      },
      "source": [
        "##### Atelectasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrxb3VL140g7"
      },
      "outputs": [],
      "source": [
        "text_prompts = [\n",
        "    'atelectasis',\n",
        "    'no atelectasis',\n",
        "]\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKkYJ3m880eN"
      },
      "source": [
        "#### Report Style Prompting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary to save prompts\n",
        "prompts = {}"
      ],
      "metadata": {
        "id": "BjcrEWoERA2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsN9PNJKKE5R"
      },
      "source": [
        "##### Healthy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLd1xRHj9aUP"
      },
      "outputs": [],
      "source": [
        "prompts['healthy'] = '''\n",
        "The lungs are clear.\n",
        "Normal heart size and shape.\n",
        "No abnormal fluid buildup.\n",
        "No visible tumors or masses. No pneumothorax.\n",
        "'''\n",
        "\n",
        "prompts['not healthy'] = '''\n",
        "There is an area of increased opacity and consolidation indicating pneumonia.\n",
        "Enlargement of the heart silhouette indicating cardiomegaly.\n",
        "There is a loss in volume indicating atelectasis.\n",
        "'''\n",
        "\n",
        "text_prompts = [\n",
        "    prompts['healthy'],\n",
        "    prompts['not healthy']\n",
        "]\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T6JNi2Qg2t2"
      },
      "source": [
        "##### Cardiomegaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQHPL2Y_hE7m"
      },
      "outputs": [],
      "source": [
        "prompts['cardiomegaly'] = '''\n",
        "Increased size of the heart shadow.\n",
        "Enlargement of the heart silhouette.\n",
        "Increased diameter of the heart border.\n",
        "Increased cardiothoracic ratio.\n",
        "'''\n",
        "prompts['no cardiomegaly'] = '''\n",
        "The heart shadow size is normal.\n",
        "The heart silhouette is normal.\n",
        "Normal diameter of the heart border.\n",
        "Normal cardiothoracic ratio.\n",
        "'''\n",
        "\n",
        "text_prompts = [\n",
        "    prompts['cardiomegaly'],\n",
        "    prompts['no cardiomegaly']\n",
        "]\n",
        "\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Pleural Effusion"
      ],
      "metadata": {
        "id": "YetBDBC-gPMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts['pleural effusion'] = '''\n",
        "Blunting of costophrenic angles.\n",
        "Opacity in the lower lung fields.\n",
        "Mediastinal shift.\n",
        "Reduced lung volume.\n",
        "Presence of meniscus sign or veil-like appearance.\n",
        "'''\n",
        "\n",
        "prompts['no pleural effusion'] = '''\n",
        "No blunting of costophrenic angles.\n",
        "No opacity in the lower lung fields.\n",
        "The lungs are clear.\n",
        "No mediastinal shift.\n",
        "No presence of meniscus sign or veil-like appearance.\n",
        "'''\n",
        "\n",
        "text_prompts = [\n",
        "    prompts['pleural effusion'],\n",
        "    prompts['no pleural effusion']\n",
        "]\n",
        "\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ],
      "metadata": {
        "id": "kq-ulYzggOVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9SJRcO6ZjWt"
      },
      "source": [
        "##### Pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xetfMFUBpI4g"
      },
      "outputs": [],
      "source": [
        "prompts['pneumonia'] = 'There is an area of increased opacity and consolidation indicating pneumonia.'\n",
        "prompts['no pneumonia'] = 'There are no opacities, no consolidation and no pleural effusion. No signs of pneumonia.'\n",
        "\n",
        "text_prompts = [\n",
        "    prompts['pneumonia'],\n",
        "    prompts['no pneumonia']\n",
        "]\n",
        "\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hh_H_zifLB0"
      },
      "source": [
        "##### Atelectasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGCk-5dxfM3e"
      },
      "outputs": [],
      "source": [
        "prompts['atelectasis'] = '''\n",
        "The lung appears reduced in volume supporting the presence of atelectasis.\n",
        "The mediastinum shows a shift, consistent with volume loss associated with atelectasis.\n",
        "These findings are consistent with atelectasis.\n",
        "'''\n",
        "\n",
        "prompts['no atelectasis'] = '''\n",
        "Both lungs are well-expanded with clear lung fields.\n",
        "The lung volumes appear normal and symmetric, with no apparent reduction in the size of either lung.\n",
        "The mediastinum is positioned centrally, without evidence of mediastinal shift.\n",
        "No radiographic signs of atelectasis are present. The lungs appear normally aerated and expanded.\n",
        "'''\n",
        "\n",
        "text_prompts = [\n",
        "    prompts['atelectasis'],\n",
        "    prompts['no atelectasis']\n",
        "]\n",
        "\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranging from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jS884s92ZDe"
      },
      "source": [
        "## Stanardized Report Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Structured reporting template definition"
      ],
      "metadata": {
        "id": "1_J2EgHLOabr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR4Px2sk2vX4"
      },
      "outputs": [],
      "source": [
        "reporting_template = {\n",
        "        'healthy' : 'The lungs are clear. No findings.',\n",
        "        'not healthy' :\n",
        "            [\n",
        "                {\n",
        "                    'no cardiomegaly' : 'The heart is normal in size.',\n",
        "                    'cardiomegaly' : 'There is cardiomegaly.',\n",
        "                },\n",
        "                {\n",
        "                    'no pleural effusion': 'There is no pleural effusion.',\n",
        "                    'pleural effusion': 'There is pleural effusion.'\n",
        "                },\n",
        "            ],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Report generation"
      ],
      "metadata": {
        "id": "hfMHavB7OhL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define recursive report generation method\n",
        "\n",
        "def generate_report(image, reporting_template):\n",
        "    report_sentences = []\n",
        "    if isinstance(reporting_template, dict):\n",
        "        choices = list(reporting_template.keys())\n",
        "        text_prompts = [prompts[k] for k in choices]\n",
        "        similarities = get_similarities_from_text_and_images(text_prompts, [image])\n",
        "        decision = choices[similarities.argmax()]\n",
        "        decision_content = reporting_template[decision]\n",
        "        if isinstance(decision_content, str):\n",
        "            report_sentences.append(decision_content)\n",
        "        else:\n",
        "            report_sentences.extend(\n",
        "                generate_report(image, decision_content)\n",
        "            )\n",
        "    elif isinstance(reporting_template, list):\n",
        "        for sub_report in reporting_template:\n",
        "            report_sentences.extend(\n",
        "                generate_report(image, sub_report)\n",
        "            )\n",
        "    return report_sentences"
      ],
      "metadata": {
        "id": "9pCUuPYWP7nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate report for all patients\n",
        "\n",
        "for i, image in enumerate(patient_images, 1):\n",
        "    print('Patient', i)\n",
        "    display(image)\n",
        "    report = ' '.join(generate_report(image, reporting_template))\n",
        "    print('Generated report:', report,'\\n\\n')"
      ],
      "metadata": {
        "id": "-60Ctpk3aSIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Open tasks\n",
        "\n",
        "1.  Why is cardiomegaly detected in patient 3 even though it is not present? How could this be fixed?\n",
        "\n",
        "2. Change the prompts and observe the change in similarities\n",
        "\n",
        "3. Add a new classification task e.g. lung opacity and come up with a report style prompt (try if ChatGPT can come up with good descriptors)\n",
        "\n",
        "2. Extend the reporting with more choices, e.g. adding diagnoses or the severity assessment of cardiomegaly"
      ],
      "metadata": {
        "id": "cfUhB4SqpuTo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uz2liYturUNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Resources"
      ],
      "metadata": {
        "id": "OgwOpR3jAiLc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHg-_yctHra"
      },
      "source": [
        "## Radiology Report retrieval\n",
        "\n",
        "Given a large database of reports the embedding of an image can be used to retrieve the reports most similar to the given image. In this example the images are compared to their matching reports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ghW5am7tYup"
      },
      "outputs": [],
      "source": [
        "# write all ground truth reports in a list as text prompts\n",
        "text_prompts = [report.replace('. ', '.\\n') for report in patient_reports]\n",
        "\n",
        "similarities = get_similarities_from_text_and_images(text_prompts, patient_images)\n",
        "print('Cosine similarities ranging from -1 to 1:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=False)\n",
        "print('\\nSoftmax probabilities ranginf from 0% to 100%:')\n",
        "plot_similarities(similarities, text_prompts, patient_images, patient_descriptions, plot_probabilities=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F56LD16AtiEt"
      },
      "source": [
        "## Phrase grounding\n",
        "\n",
        "BioVil allows for the visualization of text similarity in images referred to as \"phrase grounding\" [1]:\n",
        "\n",
        "![BioVil phrase grounding examples](https://www.microsoft.com/en-us/research/uploads/prod/2022/07/MS-CXR-2048x472.png)\n",
        "\n",
        "[Link to phrase grounding notebook](https://github.com/microsoft/hi-ml/blob/main/hi-ml-multimodal/notebooks/phrase_grounding.ipynb)\n",
        "\n",
        "[BioVil code on github](https://github.com/microsoft/hi-ml/tree/main/hi-ml-multimodal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4sl9Cer63H7"
      },
      "source": [
        "## Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis\n",
        "\n",
        "![Xplainer graphical abstract](https://raw.githubusercontent.com/ChantalMP/Xplainer/master/figures/model_overview.png)\n",
        "\n",
        "We propose a new way of explainability for zero-shot diagnosis prediction in the clinical domain. Instead of directly predicting a diagnosis, we prompt the model to classify the existence of descriptive observations, which a radiologist would look for on an X-Ray scan, and use the descriptor probabilities to estimate the likelihood of a diagnosis, making our model explainable by design. For this we leverage BioVil, a pretrained CLIP model for X-rays and apply contrastive observation-based prompting. We evaluate Xplainer on two chest X-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in improving the performance and explainability of zero-shot diagnosis.\n",
        "\n",
        "Pellegrini, Chantal, et al. \"Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis.\" accepted at MICCAI 2023.\n",
        "\n",
        "[MICCAI 2023 Paper](https://link.springer.com/chapter/10.1007/978-3-031-43904-9_41)\n",
        "\n",
        "[Huggingface Demo](https://huggingface.co/spaces/CAMP-ViL/Xplainer)\n",
        "\n",
        "[Preprint on arxiv](https://arxiv.org/abs/2303.13391)\n",
        "\n",
        "[Code on GitHub](https://github.com/ChantalMP/Xplainer)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
